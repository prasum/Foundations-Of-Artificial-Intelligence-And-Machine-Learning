# -*- coding: utf-8 -*-
"""Copy of Sentiment Analysis v2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1swS8apN_L0TXHv1lw2pqm0tQiuDFrgqb

### Deep Sentiment Analysis Tutorial

### Setup Environment
- On Google Colab make sure you select Python 3/GPU runtime before running the code

#### Choose Python 3 + GPU/CPU

<img src="https://i.stack.imgur.com/khwGc.png" width="400"></img>
<img src="https://i.stack.imgur.com/5iL6w.png" width="400"></img>
"""

# Commented out IPython magic to ensure Python compatibility.
# %env CUDA_VISIBLE_DEVICES=0

"""### Download Data"""

![ ! -d data ] && mkdir data/
![ -f data/aclImdb_v1.tar.gz ] && echo "Skip Download"
![ ! -f data/aclImdb_v1.tar.gz ] && wget -N https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data/

# Commented out IPython magic to ensure Python compatibility.
# %%time
# ![ -d data/aclImdb/ ] && echo "Data already extracted"
# ![ ! -d data/aclImdb/ ] && tar -xzf data/aclImdb_v1.tar.gz -C data/

"""### Imports"""

import os
import re
import nltk
from collections import Counter
from tqdm import tqdm_notebook
import numpy as np
import tensorflow as tf
from tensorflow.contrib import seq2seq
from tensorflow.contrib.rnn import DropoutWrapper
import random

from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score

nltk.download('punkt')

MAX_SEQ_LEN = 200
BATCH_SIZE = 1000

class Lang:
    def __init__(self, counter, vocab_size):
        self.word2id = {}
        self.id2word = {}
        self.pad = "<PAD>"
        self.sos = "<SOS>"
        self.eos = "<EOS>"
        self.unk = "<UNK>"
        
        self.ipad = 0
        self.isos = 1
        self.ieos = 2
        self.iunk = 3
        
        self.word2id[self.pad] = 0
        self.word2id[self.sos] = 1
        self.word2id[self.eos] = 2
        self.word2id[self.unk] = 3
        
        self.id2word[0] = self.pad
        self.id2word[1] = self.sos
        self.id2word[2] = self.eos
        self.id2word[3] = self.unk
        
        curr_id = 4
        for w, c in counter.most_common(vocab_size-curr_id):
            self.word2id[w] = curr_id
            self.id2word[curr_id] = w
            curr_id += 1
    
    def encodeSentence(self, wseq, max_len=-1):
        # wseq = nltk.tokenize.word_tokenize(s.lower().strip())
        if max_len == -1:
            return [self.word2id[w] if w in self.word2id else self.iunk for w in wseq]
        else:
            return ([self.word2id[w] if w in self.word2id else self.iunk for w in wseq] + [self.ieos] + [self.ipad]*max_len)[:max_len]
        
    def encodeSentence2(self, wseq, max_len=-1):
        # wseq = nltk.tokenize.word_tokenize(s.lower().strip()) 
        return min(max_len, len(wseq)+1), \
            ([self.word2id[w] if w in self.word2id else self.iunk for w in wseq] + \
                [self.ieos] + [self.ipad]*max_len)[:max_len]
    
    def decodeSentence(self, id_seq):
        id_seq = np.array(id_seq + [self.ieos])
        j = np.argmax(id_seq==self.ieos)
        s = ' '.join([self.id2word[x] for x in id_seq[:j]])
        s = s.replace(self.unk, "UNK")
        return s

"""### Let's read in the data"""

data_folder = 'data/aclImdb/'

rp = os.path.join(data_folder, 'train/pos')
train_positive = [os.path.join(rp, f) for f in os.listdir(rp)]
rp = os.path.join(data_folder, 'train/neg')
train_negative = [os.path.join(rp, f) for f in os.listdir(rp)]

rp = os.path.join(data_folder, 'test/pos')
test_positive = [os.path.join(rp, f) for f in os.listdir(rp)]
rp = os.path.join(data_folder, 'test/neg')
test_negative = [os.path.join(rp, f) for f in os.listdir(rp)]

import copy
test_category = copy.copy(test_positive)

print(len(test_category))
print('test_positive',len(test_positive))
print('test_negative',len(test_negative))

test_category.extend(test_negative)
print(len(test_category))
print('test_positive',len(test_positive))
print('test_negative',len(test_negative))

"""#### Limit number of samples
To quickly train a small model, consider setting n_train and n_test to some relatively small numbers e.g. `1000`. Set, 
`n_train = n_test = -1` to use all the samples available.
"""

n_train = 100000
n_test = 25000

re_html_cleaner = re.compile(r"<.*?>")

en_counter = Counter()
train_data = []
for _fname in tqdm_notebook(train_positive[:n_train], desc="Crunching +ve samples: "):
    with open(_fname) as f:
        text = f.read().strip()
        text = re_html_cleaner.sub(" ", text)
        wseq = nltk.tokenize.word_tokenize(text.lower())
        en_counter += Counter(wseq)
        train_data.append((wseq, 1))
        
for _fname in tqdm_notebook(train_negative[:n_train], desc="Crunching -ve samples: "):
    with open(_fname) as f:
        text = f.read().strip()
        text = re_html_cleaner.sub(" ", text)
        wseq = nltk.tokenize.word_tokenize(text.lower())
        en_counter += Counter(wseq)
        train_data.append((wseq, 0))

test_data = []
for _fname in tqdm_notebook(test_positive[:n_test], desc="Crunching +ve samples: "):
    with open(_fname) as f:
        text = f.read().strip()
        text = re_html_cleaner.sub(" ", text)
        wseq = nltk.tokenize.word_tokenize(text.lower())
        test_data.append((wseq, 1))
        
for _fname in tqdm_notebook(test_negative[:n_test], desc="Crunching -ve samples: "):
    with open(_fname) as f:
        text = f.read().strip()
        text = re_html_cleaner.sub(" ", text)
        wseq = nltk.tokenize.word_tokenize(text.lower())
        test_data.append((wseq, 0))
        
print(len(test_data))

# A few sample english words
print("\nMost common en words in dataset:\n", en_counter.most_common(10))

print("\nTotal (en)words gathered from dataset:", len(en_counter))

V = 10000

en_lang = Lang(en_counter, V)

wseq = nltk.tokenize.word_tokenize("Where are you going?".lower())
print("Test en encoding:", en_lang.encodeSentence(wseq))
print("Test en decoding:", en_lang.decodeSentence(en_lang.encodeSentence(wseq, 10)))

tf.random.set_random_seed(0)
tf.reset_default_graph()

"""### The RNN based Sentence Classifier architecture
- We will implement a RNN based classifier architecture for sentiment analysis in Tensorflow r1.13.1 / r1.14
- Debugging Tip: Always keep track of tensor dimensions!
- **Tensorflow Computation Graph** - We will build a tf computation graph first. This is the representation used by tf for any neural network architecture. Once the computation graph is built, you can feed data to it for training or inference

#### Word Embedding Matrix
"""

en_word_emb_matrix = tf.get_variable("en_word_emb_matrix", (V, 300), dtype=tf.float32)

"""#### Placeholders"""

keep_prob = tf.placeholder(tf.float32)

input_ids = tf.placeholder(tf.int32, (None, MAX_SEQ_LEN))
input_lens = tf.placeholder(tf.int32, (None, ))

y_placeholder = tf.placeholder(tf.int32, (None,))

"""#### Tensorflow Graphs"""

input_emb = tf.nn.embedding_lookup(en_word_emb_matrix, input_ids)

input_emb.shape

"""#### Encoder

##### RNN Units
"""

# Create a single GRU cell
encoder_fw_cell = tf.nn.rnn_cell.GRUCell(128)
# Add dropout : Dropout is applied to the hidden state output at every time step
encoder_fw_cell = DropoutWrapper(encoder_fw_cell, output_keep_prob=keep_prob)

encoder_bw_cell = tf.nn.rnn_cell.GRUCell(128)
encoder_bw_cell = DropoutWrapper(encoder_bw_cell, output_keep_prob=keep_prob)

# Unrolling of time-sequence
# Apply the encoder cell on input sequence and unroll computation upto
# max sequence length
enc_outputs, enc_state= tf.nn.bidirectional_dynamic_rnn(
    encoder_fw_cell,encoder_bw_cell, input_emb,sequence_length=input_lens,initial_state_fw=encoder_fw_cell.zero_state(BATCH_SIZE, dtype=tf.float32),initial_state_bw=encoder_bw_cell.zero_state(BATCH_SIZE, dtype=tf.float32),scope='blstm5')

output = tf.concat(enc_outputs, 2)

# Create a single GRU cell
encoder_fw_cell1 = tf.nn.rnn_cell.GRUCell(128)
# Add dropout : Dropout is applied to the hidden state output at every time step
encoder_fw_cell1 = DropoutWrapper(encoder_fw_cell1, output_keep_prob=keep_prob)
encoder_bw_cell1 = tf.nn.rnn_cell.GRUCell(128)
encoder_bw_cell1 = DropoutWrapper(encoder_bw_cell1, output_keep_prob=keep_prob)

enc_outputs, enc_state= tf.nn.bidirectional_dynamic_rnn(
    encoder_fw_cell1,encoder_bw_cell1,output,sequence_length=input_lens,dtype=tf.float32,scope='blstm6')

enc_state = tf.concat(enc_state, 1)

output = tf.concat(enc_outputs, 2)

output.shape

#output=output[:,-1,:]

enc_state.shape

'''with tf.variable_scope("attention") as scope:
            atn_in = tf.expand_dims(output, axis=2) # [batch_size, max_time, 1, 2 * encoder_hidden_size]
            atn_w = tf.Variable(
                tf.truncated_normal(shape=[1, 1, 2 * 64, 16], stddev=0.1),
                name="atn_w")
            atn_b = tf.Variable(tf.zeros(shape=[16]))
            atn_v = tf.Variable(
                tf.truncated_normal(shape=[1, 1, 16, 1], stddev=0.1),
                name="atn_b")
            atn_activations = tf.nn.tanh(
                tf.nn.conv2d(atn_in, atn_w, strides=[1,1,1,1], padding='SAME') + atn_b)
            atn_scores = tf.nn.conv2d(atn_activations, atn_v, strides=[1,1,1,1], padding='SAME')
            atn_probs = tf.nn.softmax(tf.squeeze(atn_scores, [2, 3]))
            _atn_out = tf.matmul(tf.expand_dims(atn_probs, 1), output)
            atn_out = tf.squeeze(_atn_out, [1], name="atn_out") '''

"""### Classifier Layer"""

# A simple fully connected linear layer
# W^T*X + b
dense_layer = tf.layers.Dense(2)
#dense_layer1 = tf.layers.Dense(128)

#atn_out=tf.expand_dims(atn_out, -1)

"""#### Approaches:
As input to the final linear layers use mean of the hidden states?

or

As input to the final linear layers use the last hidden state?

##### Approch 1: Take mean of enc_outputs across dimension 1
- **IMPORTANT:** Need to **mask** the positions in input sentence that doesn't contain any inputs
"""

'''masks = tf.sequence_mask(input_lens, MAX_SEQ_LEN, dtype=tf.float32, name='masks')
class_prob = tf.nn.sigmoid(
                dense_layer(
                   tf.reduce_mean(
                         output*masks[:,:, None], 1)
                )
) 

print(class_prob.shape)'''

#atn_out=tf.expand_dims(atn_out, -1)

#print(atn_out.shape)

"""##### Approch 2: Use enc_state (final hidden state)"""

class_prob = tf.nn.sigmoid(dense_layer(enc_state))
print(class_prob.shape)

"""#### Loss and Optimizers [softmax_cross_entropy]
Note that `onehot_labels` and `logits` must have the same shape, e.g. `[batch_size, num_classes]`
"""

print(y_placeholder.shape)
print(class_prob.shape)

# Loss function - softmax cross entropy
y_ = tf.cast(y_placeholder[:, None], dtype=tf.float32)
cost = -y_*tf.log(class_prob + 1e-12) - (1-y_)*tf.log(1-class_prob + 1e-12)
cost = tf.reduce_mean(cost)

# Optimizer
optimizer = tf.train.AdamOptimizer(0.0001)

train_op = optimizer.minimize(cost)

init = tf.global_variables_initializer()

"""#### Tensorflow Sessions"""

sess_config = tf.ConfigProto()
sess_config.gpu_options.allow_growth = True

sess = tf.InteractiveSession(config=sess_config)
sess.run(init)

"""#### Minibatch Training"""

random.seed(41)

random.shuffle(train_data)

train_n = len(train_data)

test_n = len(test_data)

def small_test():
    all_true = []
    all_preds = []
    for m in range(0, test_n, BATCH_SIZE):
        n = m + BATCH_SIZE
        if n > test_n:
            break

        input_batch = np.zeros((BATCH_SIZE, MAX_SEQ_LEN), dtype=np.int32)
        input_lens_batch = np.zeros((BATCH_SIZE,), dtype=np.int32)
        true_class_batch = np.zeros((BATCH_SIZE))
        for i in range(m, n):
            b,a = en_lang.encodeSentence2(test_data[i][0], MAX_SEQ_LEN)
            input_batch[i-m,:] = a
            input_lens_batch[i-m] = b
            true_class_batch[i-m] = test_data[i][1]

        feed_dict={
            input_ids: input_batch,
            input_lens: input_lens_batch,
            keep_prob: 1.0
        }
        pred_batch = sess.run(class_prob, feed_dict=feed_dict)
        # acc = accuracy_score(true_class_batch, pred_batch > 0.5)
        all_true.extend(list(true_class_batch))
        all_preds.extend(list(pred_batch[:,0]))
    
    all_true = np.array(all_true)
    all_preds = np.array(all_preds)
    print(len(all_preds))
    prec = precision_score(all_true, all_preds > 0.5)*100
    rec = recall_score(all_true, all_preds > 0.5)*100
    f1 = f1_score(all_true, all_preds > 0.5)*100
    print(f"Precision: {prec:2.2F}, Recall: {rec:2.2F}, F1-Score: {f1:2.2F}")
    return (test_category,all_preds)

for _e in range(5):
    # Mix things up a bit.
    random.shuffle(train_data)
    pbar = tqdm_notebook(range(0, train_n, BATCH_SIZE))
    batch_loss = 0
    bxi = 0
    for m in pbar:
        n = m + BATCH_SIZE
        if n <= train_n:
            # print("Epoch Complete... \n")

            input_batch = np.zeros((BATCH_SIZE, MAX_SEQ_LEN), dtype=np.int32)
            input_lens_batch = np.zeros((BATCH_SIZE,), dtype=np.int32)
            true_class_batch = np.zeros((BATCH_SIZE))
            for i in range(m, n):
                b,a = en_lang.encodeSentence2(train_data[i][0], MAX_SEQ_LEN)
                input_batch[i-m,:] = a
                input_lens_batch[i-m] = b
                true_class_batch[i-m] = train_data[i][1]

            feed_dict={
                input_ids: input_batch,
                input_lens: input_lens_batch,
                y_placeholder: true_class_batch,
                keep_prob: 0.7
            }
            sess.run(train_op, feed_dict=feed_dict)
            batch_loss += sess.run(cost, feed_dict=feed_dict)
            pbar.set_description(f"Epoch: {_e} >> Loss: {batch_loss/(bxi+1):2.2F}:")
            bxi += 1
            if (1 + n//BATCH_SIZE) % 10 == 0:
                small_test()

"""### Improving Further
- This was a very simple RNN based model for the task.
- You can still improve it a lot by tweaking hyperparameters e.g.
 - lstm size 
 - dropout
 - learning rate 
- or modifying the architecture e.g.
 - Add bidirectional RNNs
 - Use multiple layers of RNN cells
 - Add more hidden layers to the classifier
"""

filename_list1,probability_list2=small_test()
import csv
from itertools import zip_longest
data=[filename_list1, probability_list2]
export_data = zip_longest(*data, fillvalue = '')
with open('submission.csv', 'w', encoding="ISO-8859-1", newline='') as myfile:
      wr = csv.writer(myfile)
      wr.writerow(("filename", "prob_positive"))
      wr.writerows(export_data)
myfile.close()

!zip submission.csv.zip submission.csv -9